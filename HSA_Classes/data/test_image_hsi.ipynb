{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a150fd",
   "metadata": {},
   "source": [
    "# Hyper Spectral Anomaly Detection\n",
    "## Image Data Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61335015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "path = \"../\"\n",
    "sys.path.append(path)\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "from loguru import logger\n",
    "import Model as hsa_model\n",
    "import DataSet as hsa_dataset\n",
    "import Viz as hsa_viz\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sink=f\"HSA_log.log\", level=\"CRITICAL\")\n",
    "import MultiFilter as hsa_multifilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1828202",
   "metadata": {},
   "source": [
    "## Image \n",
    "Read Images and plot shape of np.array associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./night_sky.jpeg\"\n",
    "img = Image.open(image_path)\n",
    "array = np.array(img)\n",
    "print(f\"array shape: {array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968485f2",
   "metadata": {},
   "source": [
    "Plot images of the RGB spectra and original data as np before conversion to df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a622cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "titles = [\"Red Spectra\", \"Blue Spectra\", \"Green Spectra\", \"Original Image\"]\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    try:\n",
    "        sns.heatmap(\n",
    "            array[:, :, i],\n",
    "            square=True,\n",
    "            xticklabels=False,\n",
    "            yticklabels=False,\n",
    "            cbar=False,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(titles[i])\n",
    "    except:\n",
    "        ax.set_title(titles[i])\n",
    "        image = mpimg.imread(image_path)\n",
    "        ax.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf6174",
   "metadata": {},
   "source": [
    "Generate a list of data points from image and convert to df to begin vanilla preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "columns = [\"red\", \"green\", \"blue\"]\n",
    "\n",
    "for row in range(len(array)):\n",
    "    try:\n",
    "        data = np.append(data, array[row, :, :], axis=0)\n",
    "    except:\n",
    "        data = array[row, :, :]\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(data)\n",
    "preprocessed_df = pd.DataFrame(scaler.transform(data), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229b7ea",
   "metadata": {},
   "source": [
    "## Instantiate\n",
    "Instantiate the model and dataloader with preprocessed data as a np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_std_tolerance = 1.2\n",
    "penalty_ratio = 0.99\n",
    "cutoff_distance = 1\n",
    "converge_toll=1e-5,\n",
    "\n",
    "model = hsa_model.HSA_model(\n",
    "    penalty_ratio = penalty_ratio,\n",
    "    cutoff_distance = cutoff_distance,\n",
    "    converge_toll = converge_toll,\n",
    "    anomaly_std_tolerance = anomaly_std_tolerance,\n",
    "    logger=logger,\n",
    "    affinity_matrix_iterations = 20,\n",
    "    lr = 2.7,\n",
    "    multifilter_flag = 0,\n",
    ")\n",
    "dataset = hsa_dataset.HSA_dataset(\n",
    "    preprocessed_np=preprocessed_df.to_numpy(), logger=logger\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0e333",
   "metadata": {},
   "source": [
    "## \"Train\" \n",
    "Train model on preprocessed image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = \"Images/\"\n",
    "log_directory = f\"{base_directory}/logs/\"\n",
    "results_directory = f\"{base_directory}/results/\"\n",
    "model.set_directories(log_directory, results_directory)\n",
    "\n",
    "model.preprocessed_df = preprocessed_df\n",
    "\n",
    "logger.info(\"Starting to run through the dataloader on initial pass.\")\n",
    "unique_id_str = \"test\"\n",
    "batch_size = 1000\n",
    "iterations = 10000\n",
    "\n",
    "for i, data in enumerate(dataloader):  # setting up gpus\n",
    "    model.set_trial(i * batch_size, batch_size, unique_id_str)\n",
    "\n",
    "    # Model set up and weight generation\n",
    "    model.read_data(\n",
    "        data_multifilter_df=data.squeeze(0)\n",
    "    ).vertex_weights_distances().weight_generation().graph_evolution()\n",
    "\n",
    "    model.train(iterations=iterations)\n",
    "    model.infer(preprocessed_df)\n",
    "\n",
    "    # Store anomalous predictions throughout all batches for use in multi filter\n",
    "    try:\n",
    "        total_anomaly_index = np.append(total_anomaly_index, model.anomaly_index_raw)\n",
    "    except:\n",
    "        total_anomaly_index = model.anomaly_index_raw\n",
    "    # if i >100:\n",
    "    #     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1431f",
   "metadata": {},
   "source": [
    "Collect $1^{st}$ rank anomalies for later use in the Multifilter. Non-anomalous data is collected from all data such that anomalies and non-anomalous data compose $\\%10$ and $\\%90$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c946df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_index, mix_data, anomaly_index = model.global_collect_multifilter_df(\n",
    "    preprocessed_df.to_numpy(),\n",
    "    total_anomaly_index[: len(preprocessed_df.to_numpy())].astype(int),\n",
    "    mf_batch_size=9 * len(total_anomaly_index),\n",
    ")\n",
    "anomaly_prediction_frequency_df = model.apf_df_generation(anomaly_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bae956",
   "metadata": {},
   "source": [
    "Shuffle data so that the blocks of anomalous and non- anomalous data are randomly distributed throughout the multifilter dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b05c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.uni_shuffle_multifilter_df(\n",
    "    mix_index.astype(int), mix_data.astype(int), anomaly_index.astype(int)\n",
    ")\n",
    "mf_data = model.all_data\n",
    "logger.debug(\"Anomalous data has been colleted into first multifilter dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6eedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Original data input - preprocessed_df: {preprocessed_df.shape} \\nFirst rank anomalous predictions - anomaly_prediction_frequency_df:{anomaly_prediction_frequency_df.shape} \\nMultifilter data shape - mf_data: {mf_data.shape}, {type(mf_data)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02683b9",
   "metadata": {},
   "source": [
    "## Multifilter\n",
    "Instantiating a multifilter based off of the first pass model's hyper parameters. After the model has predicted on the multifilter data set thirty times, an updated anomaly_prediction_frequency_df is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_filters = 10\n",
    "all_index_user = model.all_index_user\n",
    "MF_model, anomaly_prediction_frequency_df = hsa_multifilter.multifilter(\n",
    "    multi_filters,\n",
    "    batch_size,\n",
    "    penalty_ratio,\n",
    "    cutoff_distance,\n",
    "    anomaly_std_tolerance,\n",
    "    mf_data,\n",
    "    all_index_user,\n",
    "    model,\n",
    ")\n",
    "display(anomaly_prediction_frequency_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208448eb",
   "metadata": {},
   "source": [
    "## Visualization \n",
    "The HSA_viz class is used to consistently inspect the model's preprocessed input data, model scores, and multifilter prediction frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = hsa_viz.HSA_viz(\n",
    "    MF_model.m,\n",
    "    MF_model.preprocessed_np,\n",
    "    batch_size,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    \"plot_directory\",\n",
    "    \"unique_id_str\",\n",
    "    logger,\n",
    ")\n",
    "viz.heatmap_bin_predictions_vert(\n",
    "    MF_model.bin_score,\n",
    "    MF_model.anomalous_location,\n",
    "    MF_model.anomaly_index_raw,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac153ddd",
   "metadata": {},
   "source": [
    "Insert non-zero multifilter frequency of predictions to preprocessed_df to visualize model predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd8c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df[\"Anomaly Bin Count\"] = np.zeros(len(preprocessed_df))\n",
    "for i in anomaly_prediction_frequency_df[\n",
    "    anomaly_prediction_frequency_df[\"Anomaly Bin Count\"] > 0\n",
    "].index:\n",
    "    preprocessed_df.loc[anomaly_prediction_frequency_df.loc[i], \"Anomaly Bin Count\"] = (\n",
    "        anomaly_prediction_frequency_df.loc[i, \"Anomaly Bin Count\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb017df1",
   "metadata": {},
   "source": [
    "Restore data and predictions to original image dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3faf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 1280\n",
    "anomaly_score_np = []\n",
    "for i in range(int(len(preprocessed_df) / image_width)):\n",
    "    anomaly_score_np.append(\n",
    "        preprocessed_df[\"Anomaly Bin Count\"][i * image_width : (1 + i) * image_width]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a9d6b",
   "metadata": {},
   "source": [
    "## Comparison between model predictions and raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "titles = [\"Red Spectra\", \"Blue Spectra\", \"Green Spectra\", \"Anomaly Scores\"]\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    try:\n",
    "        sns.heatmap(\n",
    "            array[:, :, i],\n",
    "            square=True,\n",
    "            xticklabels=False,\n",
    "            yticklabels=False,\n",
    "            cbar=False,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(titles[i])\n",
    "    except:\n",
    "        ax.set_title(titles[i])\n",
    "        sns.heatmap(\n",
    "            anomaly_score_np,\n",
    "            square=True,\n",
    "            xticklabels=False,\n",
    "            yticklabels=False,\n",
    "            cbar=False,\n",
    "            ax=ax,\n",
    "            cmap=\"rocket_r\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febf3f2",
   "metadata": {},
   "source": [
    "Model prediction statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max anomaly score: {np.array(anomaly_score_np).max()}\")\n",
    "print(f\"Number of Anomalies:  {np.sum(np.greater(np.array(anomaly_score_np), 0))}\")\n",
    "sns.histplot(\n",
    "    preprocessed_df.loc[preprocessed_df[\"Anomaly Bin Count\"] > 0][\"Anomaly Bin Count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = \"../\"\n",
    "sys.path.append(path)\n",
    "import Pipeline\n",
    "from loguru import logger\n",
    "import pandas as pd \n",
    "\n",
    "penalty_ratio = 0.9\n",
    "cutoff_distance = 1.2\n",
    "lr = 2.7\n",
    "anomaly_std_tolerance = 1.2\n",
    "bin_count = 3\n",
    "max_spawn_dummies = 30\n",
    "percent_variance_explained = 1\n",
    "min_additional_percent_variance_exp = 0\n",
    "logging_level = \"TRACE\"\n",
    "\n",
    "pipe = Pipeline.HSA_pipeline(\n",
    "    penalty_ratio,\n",
    "    cutoff_distance,\n",
    "    lr,\n",
    "    anomaly_std_tolerance,\n",
    "    bin_count,\n",
    "    max_spawn_dummies,\n",
    "    percent_variance_explained,\n",
    "    min_additional_percent_variance_exp,\n",
    "    logger=logger,\n",
    "    logging_level=\"DEBUG\",\n",
    "    base_directory=\"./\",\n",
    "    num_workers=0,\n",
    "    unique_id_str= \"\"\n",
    ")\n",
    "\n",
    "pipe.pipeline(preprocessed_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536f693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
