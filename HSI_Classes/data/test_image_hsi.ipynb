{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61335015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "path=\"../\"\n",
    "sys.path.append(path)\n",
    "import Preprocessing as hsa_preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "from loguru import logger\n",
    "import Model as hsa_model\n",
    "import DataLoader as hsa_dataloader\n",
    "import Viz as hsa_viz\n",
    "from torch.utils.data import DataLoader\n",
    "logger.add(sink=f\"HSI_log.log\",\n",
    "            level=\"CRITICAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1828202",
   "metadata": {},
   "source": [
    "Read Images and plot shape of np.array associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path= \"./night_sky2.jpeg\"\n",
    "img = Image.open(image_path)\n",
    "array = np.array(img)\n",
    "print(f\"array shape: {array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968485f2",
   "metadata": {},
   "source": [
    "Plot images of the RGB spectra and original data as np before conversion to df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a622cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "titles= [\"Red Spectra\", \"Blue Spectra\",\"Green Spectra\", \"Original Image\"]\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    try:\n",
    "        sns.heatmap(array[:,:,i], square=True, xticklabels=False, yticklabels=False, cbar=False, ax=ax)\n",
    "        ax.set_title(titles[i])\n",
    "    except:\n",
    "        ax.set_title(titles[i])\n",
    "        image = mpimg.imread(image_path)\n",
    "        ax.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf6174",
   "metadata": {},
   "source": [
    "Generate a list of data points from image and convert to df to begin vanilla preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "columns = [\"red\", \"green\", \"blue\"]\n",
    "\n",
    "for row in range(len(array)):\n",
    "    try:\n",
    "        data = np.append(data, array[row, : , :], axis=0)\n",
    "    except:\n",
    "        data = array[row, : , :]\n",
    " \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "preprocessed_df = pd.DataFrame(scaler.transform(data), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229b7ea",
   "metadata": {},
   "source": [
    "Instantiate the model and dataloader with preprocessed data as a np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_std_toll= 1\n",
    "penalty_ratio=.3\n",
    "cutoff_dist=2\n",
    "model = hsa_model.HSI_model(\n",
    "            penalty_ratio=penalty_ratio,\n",
    "            cutoff_dist= cutoff_dist,\n",
    "            converge_toll= 1e-5,\n",
    "            anomaly_std_toll= anomaly_std_toll,\n",
    "            affinity_matrix_iterations= 20,\n",
    "            lr= 2.7,\n",
    "            logger=logger,\n",
    "            multifilter_flag=0,\n",
    "        )\n",
    "dataset = hsa_dataloader.HSI_dataset(preprocessed_np=preprocessed_df.to_numpy(), logger=logger)\n",
    "dataloader = DataLoader(dataset, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = \"Images/\"\n",
    "log_directory = f\"{base_directory}/logs/\"\n",
    "results_directory = f\"{base_directory}/results/\"\n",
    "model.set_directories(log_directory, results_directory)\n",
    "\n",
    "logger.info(\"Starting to run through the dataloader on initial pass.\")\n",
    "num_samples = 1000\n",
    "unique_id_str = \"test\"\n",
    "iterations = 10000\n",
    "\n",
    "for i, data in enumerate(dataloader):  # setting up gpus\n",
    "    model.set_trial(i * num_samples, num_samples, unique_id_str)\n",
    "    # Model set up and weight generation\n",
    "    model.readData(\n",
    "        data_multifilter_df=data.squeeze(0)\n",
    "    ).vertWeights_distances().affinityGen().graphEvo()\n",
    "    # Training steps\n",
    "    model.torch_optimize_POF(iterations=iterations)\n",
    "    # Prediction step\n",
    "    model.model_Predictions(preprocessed_df)\n",
    "    # Store anomalous predictions throughout all batches for use in multi filter\n",
    "    try:\n",
    "        total_anomaly_index = np.append(total_anomaly_index, model.x_label)\n",
    "    except: \n",
    "        total_anomaly_index = model.x_label\n",
    "        print(f\"only on 0 :{i}\")\n",
    "    if i > 1000:\n",
    "        print(i)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c946df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_index, mix_data, anomaly_index = model.global_collect_multifilter_df(\n",
    "            preprocessed_df.to_numpy(),\n",
    "            total_anomaly_index[: len(preprocessed_df.to_numpy())].astype(int),\n",
    "            mf_num_samples=9 * len(total_anomaly_index),\n",
    "        )\n",
    "anomaly_pred_freq_df = pd.DataFrame()\n",
    "anomaly_pred_freq_df[\"User DF Index\"] = anomaly_index\n",
    "anomaly_pred_freq_df.set_index(\"User DF Index\")\n",
    "anomaly_pred_freq_df[\"Anom Pred Count\"] = np.zeros(len(anomaly_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b05c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.uni_shuffle_multifilter_df(\n",
    "            mix_index.astype(int), mix_data.astype(int), anomaly_index.astype(int)\n",
    "        )\n",
    "mf_data = model.all_data\n",
    "logger.debug(\n",
    "    \"Anomalous data has been colleted into first multifilter dataset.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6eedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original data input- preprocessed_df: {preprocessed_df.shape} \\nFirst rank anomlus predictions- anomaly_pred_freq_df:{anomaly_pred_freq_df.shape} \\nMultifilter data shape- mf_data: {mf_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02683b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82139d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_filters = 5\n",
    "user_location = model.all_index_user\n",
    "\n",
    "for i in range(multi_filters):\n",
    "    batch_dataset = hsa_dataloader.HSI_dataset(mf_data, logger)\n",
    "    batch_loader = DataLoader(\n",
    "        batch_dataset, batch_size=num_samples, \n",
    "    )\n",
    "    j = 0\n",
    "    for data in batch_loader:\n",
    "        print(data.shape)\n",
    "        # # Set up multi filter model\n",
    "        MF_model = hsa_model.HSI_model(\n",
    "            penalty_ratio=penalty_ratio,\n",
    "            cutoff_dist= cutoff_dist,\n",
    "            converge_toll= 1e-5,\n",
    "            anomaly_std_toll= anomaly_std_toll,\n",
    "            affinity_matrix_iterations= 20,\n",
    "            lr= 2.7,\n",
    "            logger=logger,\n",
    "            multifilter_flag=1,\n",
    "        )\n",
    "        MF_model.set_directories(\n",
    "            log_directory, results_directory\n",
    "        )\n",
    "        MF_model.set_trial(j * len(data), len(data), unique_id_str)\n",
    "        MF_model.readData(\n",
    "            data_multifilter_df=data.squeeze(0)\n",
    "        ).vertWeights_distances().affinityGen().graphEvo()\n",
    "\n",
    "        # # Train MF_MODEL\n",
    "        MF_model.torch_optimize_POF(iterations=iterations)\n",
    "        MF_model.model_Predictions(\n",
    "            preprocessed_df, multifilter_flag=1, user_location=user_location\n",
    "        )\n",
    "        j += 1\n",
    "        if j > 3:\n",
    "            break\n",
    "        anomaly_pred_freq_df.loc[\n",
    "            anomaly_pred_freq_df[\"User DF Index\"].isin(MF_model.x_label),\n",
    "            \"Anom Pred Count\",\n",
    "        ] += 1\n",
    "\n",
    "        logger.trace(\n",
    "            f\"Multifilter {i} of {multi_filters} multifilters is complete.\"\n",
    "        )\n",
    "        # # Global multifilter\n",
    "        mix_index, mix_data, anomaly_index = (\n",
    "            model.global_collect_multifilter_df(\n",
    "                preprocessed_df.to_numpy(),\n",
    "                total_anomaly_index[: len(preprocessed_df.to_numpy())].astype(int),\n",
    "                mf_num_samples=9 * len(total_anomaly_index),\n",
    "            )\n",
    "        )\n",
    "        MF_model.uni_shuffle_multifilter_df(mix_index, mix_data, anomaly_index)\n",
    "        mf_data = MF_model.all_data\n",
    "        user_location = MF_model.all_index_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = hsa_viz.HSI_viz(\n",
    "    MF_model.m,\n",
    "    MF_model.preprocessed_np,\n",
    "    num_samples,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    \"plot_directory\",\n",
    "    \"unique_id_str\",\n",
    "    logger,\n",
    ")\n",
    "viz.heatmap_bin_predictions_vert(\n",
    "    MF_model.bin_score,\n",
    "    MF_model.x_ticks,\n",
    "    MF_model.x_label,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd8c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df[\"Anom Pred Count\"] = np.zeros(len(preprocessed_df))\n",
    "for i in anomaly_pred_freq_df[anomaly_pred_freq_df[\"Anom Pred Count\"]>0].index:\n",
    "    preprocessed_df.loc[anomaly_pred_freq_df.loc[i], \"Anom Pred Count\"] = anomaly_pred_freq_df.loc[i,\"Anom Pred Count\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3faf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 1280\n",
    "anomaly_score_np = []\n",
    "for i in range(int(len(preprocessed_df)/image_width)):\n",
    "    anomaly_score_np.append(preprocessed_df[\"Anom Pred Count\"][i*image_width:(1+i)*image_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "titles= [\"Red Spectra\", \"Blue Spectra\",\"Green Spectra\", \"Anomaly Scores\"]\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    try:\n",
    "        sns.heatmap(array[:,:,i], square=True, xticklabels=False, yticklabels=False, cbar=False, ax=ax)\n",
    "        ax.set_title(titles[i])\n",
    "    except:\n",
    "        ax.set_title(titles[i])\n",
    "        sns.heatmap(anomaly_score_np, square=True, xticklabels=False, yticklabels=False, cbar=False, ax=ax)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22846a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(anomaly_score_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efe249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HSA_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
